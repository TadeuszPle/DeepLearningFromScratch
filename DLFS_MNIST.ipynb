{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_same_shape(array: ndarray,\n",
    "                      array_grad: ndarray):\n",
    "    assert array.shape == array_grad.shape, \\\n",
    "        '''\n",
    "        Two ndarrays should have the same shape;\n",
    "        instead, first ndarray's shape is {0}\n",
    "        and second ndarray's shape is {1}.\n",
    "        '''.format(tuple(array_grad.shape), tuple(array.shape))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    '''\n",
    "    Base class for an \"operation\" in a neural network.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_: ndarray):\n",
    "        '''\n",
    "        Stores input in the self._input instance variable\n",
    "        Calls the self._output() function.\n",
    "        '''\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output()\n",
    "\n",
    "        return self.output\n",
    "\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad() function.\n",
    "        Checks that the appropriate shapes match.\n",
    "        '''\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        return self.input_grad\n",
    "\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        The _output method must be defined for each Operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The _input_grad method must be defined for each Operation\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    '''\n",
    "    An Operation with parameters.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, param: ndarray) -> ndarray:\n",
    "        '''\n",
    "        The ParamOperation method\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Calls self._input_grad and self._param_grad.\n",
    "        Checks appropriate shapes.\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "\n",
    "        assert_same_shape(self.input_, self.input_grad)\n",
    "        assert_same_shape(self.param, self.param_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Every subclass of ParamOperation must implement _param_grad.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific `Operation`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightMultiply(ParamOperation):\n",
    "    '''\n",
    "    Weight multiplication operation for a neural network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, W: ndarray):\n",
    "        '''\n",
    "        Initialize Operation with self.param = W.\n",
    "        '''\n",
    "        super().__init__(W)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return np.dot(self.input_, self.param)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray)  -> ndarray:\n",
    "        '''\n",
    "        Compute parameter gradient.\n",
    "        '''        \n",
    "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasAdd(ParamOperation):\n",
    "    '''\n",
    "    Compute bias addition.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 B: ndarray):\n",
    "        '''\n",
    "        Initialize Operation with self.param = B.\n",
    "        Check appropriate shape.\n",
    "        '''\n",
    "        assert B.shape[0] == 1\n",
    "        \n",
    "        super().__init__(B)\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return self.input_ + self.param\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        return np.ones_like(self.input_) * output_grad\n",
    "\n",
    "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute parameter gradient.\n",
    "        '''\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Operation):\n",
    "    '''\n",
    "    Sigmoid activation function.\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''\n",
    "        Compute output.\n",
    "        '''\n",
    "        return 1.0/(1.0+np.exp(-1.0 * self.input_))\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Compute input gradient.\n",
    "        '''\n",
    "        sigmoid_backward = self.output * (1.0 - self.output)\n",
    "        input_grad = sigmoid_backward * output_grad\n",
    "        return input_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Operation):\n",
    "    \n",
    "    #hyperbolic tangent activation\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        return np.tanh(self.input_)\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "\n",
    "        return output_grad * (1 - self.output * self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Operation):\n",
    "    '''\n",
    "    \"Identity\" activation function\n",
    "    '''\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''        \n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> ndarray:\n",
    "        '''Pass through'''\n",
    "        return self.input_\n",
    "\n",
    "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
    "        '''Pass through'''\n",
    "        return output_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Layer` and `Dense`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    '''\n",
    "    A \"layer\" of neurons in a neural network.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 neurons: int):\n",
    "        '''\n",
    "        The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer\n",
    "        '''\n",
    "        self.neurons = neurons\n",
    "        self.first = True\n",
    "        self.params: List[ndarray] = []\n",
    "        self.param_grads: List[ndarray] = []\n",
    "        self.operations: List[Operation] = []\n",
    "\n",
    "    def _setup_layer(self, num_in: int) -> None:\n",
    "        '''\n",
    "        The _setup_layer function must be implemented for each layer\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input_: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes input forward through a series of operations\n",
    "        ''' \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "\n",
    "        self.input_ = input_\n",
    "\n",
    "        for operation in self.operations:\n",
    "\n",
    "            input_ = operation.forward(input_)\n",
    "\n",
    "        self.output = input_\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, output_grad: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes output_grad backward through a series of operations\n",
    "        Checks appropriate shapes\n",
    "        '''\n",
    "\n",
    "        assert_same_shape(self.output, output_grad)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "\n",
    "        input_grad = output_grad\n",
    "        \n",
    "        self._param_grads()\n",
    "\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grads(self) -> ndarray:\n",
    "        '''\n",
    "        Extracts the _param_grads from a layer's operations\n",
    "        '''\n",
    "\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "\n",
    "    def _params(self) -> ndarray:\n",
    "        '''\n",
    "        Extracts the _params from a layer's operations\n",
    "        '''\n",
    "\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    '''\n",
    "    A fully connected layer which inherits from \"Layer\"\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 neurons: int,\n",
    "                 activation: Operation = Sigmoid()):\n",
    "        '''\n",
    "        Requires an activation function upon initialization\n",
    "        '''\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    def _setup_layer(self, input_: ndarray) -> None:\n",
    "        '''\n",
    "        Defines the operations of a fully connected layer.\n",
    "        '''\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.params = []\n",
    "\n",
    "        # weights\n",
    "        self.params.append(np.random.randn(input_.shape[1], self.neurons))\n",
    "\n",
    "        # bias\n",
    "        self.params.append(np.random.randn(1, self.neurons))\n",
    "\n",
    "        self.operations = [WeightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Loss` and `MeanSquaredError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''\n",
    "    The \"loss\" of a neural network\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''Pass'''\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
    "        '''\n",
    "        Computes the actual loss value\n",
    "        '''\n",
    "        assert_same_shape(prediction, target)\n",
    "\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "\n",
    "        loss_value = self._output()\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self) -> ndarray:\n",
    "        '''\n",
    "        Computes gradient of the loss value with respect to the input to the loss function\n",
    "        '''\n",
    "        self.input_grad = self._input_grad()\n",
    "\n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "\n",
    "        return self.input_grad\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Every subclass of \"Loss\" must implement the _output function.\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Every subclass of \"Loss\" must implement the _input_grad function.\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(Loss):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__()\n",
    "\n",
    "    def _output(self) -> float:\n",
    "        '''\n",
    "        Computes the per-observation squared error loss\n",
    "        '''\n",
    "        loss = (\n",
    "            np.sum(np.power(self.prediction - self.target, 2)) / \n",
    "            self.prediction.shape[0]\n",
    "        )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "        '''\n",
    "        Computes the loss gradient with respect to the input for MSE loss\n",
    "        '''        \n",
    "\n",
    "        return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float=1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.single_class = False\n",
    "\n",
    "    def _output(self) -> float:\n",
    "\n",
    "        # applying the softmax function to each row (observation)\n",
    "        softmax_preds = softmax(self.prediction, axis=1)\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        softmax_cross_entropy_loss = (\n",
    "            -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "                (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "        )\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss) / self.prediction.shape[0]\n",
    "\n",
    "    def _input_grad(self) -> ndarray:\n",
    "\n",
    "        # if \"single_class\", \"un-normalize\" probabilities before returning gradient:\n",
    "        if self.single_class:\n",
    "            return unnormalize(self.softmax_preds - self.target)\n",
    "        else:\n",
    "            return (self.softmax_preds - self.target) / self.prediction.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NeuralNetwork`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    The class for a neural network.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[Layer],\n",
    "                 loss: Loss,\n",
    "                 seed: int = 1) -> None:\n",
    "        '''\n",
    "        Neural networks need layers, and a loss.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "        self.seed = seed\n",
    "        if seed:\n",
    "            for layer in self.layers:\n",
    "                setattr(layer, \"seed\", self.seed)        \n",
    "\n",
    "    def forward(self, x_batch: ndarray) -> ndarray:\n",
    "        '''\n",
    "        Passes data forward through a series of layers.\n",
    "        '''\n",
    "        x_out = x_batch\n",
    "        for layer in self.layers:\n",
    "            x_out = layer.forward(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def backward(self, loss_grad: ndarray) -> None:\n",
    "        '''\n",
    "        Passes data backward through a series of layers.\n",
    "        '''\n",
    "\n",
    "        grad = loss_grad\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "        return None\n",
    "\n",
    "    def train_batch(self,\n",
    "                    x_batch: ndarray,\n",
    "                    y_batch: ndarray) -> float:\n",
    "        '''\n",
    "        Passes data forward through the layers.\n",
    "        Computes the loss.\n",
    "        Passes data backward through the layers.\n",
    "        '''\n",
    "        \n",
    "        predictions = self.forward(x_batch)\n",
    "\n",
    "        loss = self.loss.forward(predictions, y_batch)\n",
    "\n",
    "        self.backward(self.loss.backward())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        '''\n",
    "        Gets the parameters for the network.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.params\n",
    "\n",
    "    def param_grads(self):\n",
    "        '''\n",
    "        Gets the gradient of the loss with respect to the parameters for the network.\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            yield from layer.param_grads    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Optimizer` and `SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    '''\n",
    "    Base class for a neural network optimizer.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01):\n",
    "        '''\n",
    "        Every optimizer must have an initial learning rate.\n",
    "        '''\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self) -> None:\n",
    "        '''\n",
    "        Every optimizer must implement the \"step\" function.\n",
    "        '''\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    Stochasitc gradient descent optimizer.\n",
    "    '''    \n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01) -> None:\n",
    "        '''Pass'''\n",
    "        super().__init__(lr)\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment \n",
    "        based on the learning rate.\n",
    "        '''\n",
    "        for (param, param_grad) in zip(self.net.params(),\n",
    "                                       self.net.param_grads()):\n",
    "\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Trainer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "class Trainer(object):\n",
    "    '''\n",
    "    Trains a neural network\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 net: NeuralNetwork,\n",
    "                 optim: Optimizer) -> None:\n",
    "        '''\n",
    "        Requires a neural network and an optimizer in order for training to occur. \n",
    "        Assign the neural network as an instance variable to the optimizer.\n",
    "        '''\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = 1e9\n",
    "        setattr(self.optim, 'net', self.net)\n",
    "        \n",
    "    def generate_batches(self,\n",
    "                         X: ndarray,\n",
    "                         y: ndarray,\n",
    "                         size: int = 32) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        Generates batches for training \n",
    "        '''\n",
    "        assert X.shape[0] == y.shape[0], \\\n",
    "        '''\n",
    "        features and target must have the same number of rows, instead\n",
    "        features has {0} and target has {1}\n",
    "        '''.format(X.shape[0], y.shape[0])\n",
    "\n",
    "        N = X.shape[0]\n",
    "\n",
    "        for ii in range(0, N, size):\n",
    "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
    "\n",
    "            yield X_batch, y_batch\n",
    "\n",
    "            \n",
    "    def fit(self, X_train: ndarray, y_train: ndarray,\n",
    "            X_test: ndarray, y_test: ndarray,\n",
    "            epochs: int=100,\n",
    "            eval_every: int=10,\n",
    "            batch_size: int=32,\n",
    "            seed: int = 1,\n",
    "            restart: bool = True)-> None:\n",
    "        '''\n",
    "        Fits the neural network on the training data for a certain number of epochs.\n",
    "        Every \"eval_every\" epochs, it evaluated the neural network on the testing data.\n",
    "        '''\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "\n",
    "            self.best_loss = 1e9\n",
    "\n",
    "        for e in range(epochs):\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "                \n",
    "                # for early stopping\n",
    "                last_model = deepcopy(self.net)\n",
    "\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "\n",
    "            batch_generator = self.generate_batches(X_train, y_train,\n",
    "                                                    batch_size)\n",
    "\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "\n",
    "                self.optim.step()\n",
    "\n",
    "            if (e+1) % eval_every == 0:\n",
    "\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
    "                    self.net = last_model\n",
    "                    # ensure self.optim is still updating self.net\n",
    "                    setattr(self.optim, 'net', self.net)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true: ndarray, y_pred: ndarray):\n",
    "    '''\n",
    "    Compute mean absolute error for a neural network.\n",
    "    '''    \n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def rmse(y_true: ndarray, y_pred: ndarray):\n",
    "    '''\n",
    "    Compute root mean squared error for a neural network.\n",
    "    '''\n",
    "    return np.sqrt(np.mean(np.power(y_true - y_pred, 2)))\n",
    "\n",
    "def eval_regression_model(model: NeuralNetwork,\n",
    "                          X_test: ndarray,\n",
    "                          y_test: ndarray):\n",
    "    '''\n",
    "    Compute mae and rmse for a neural network.\n",
    "    '''\n",
    "    preds = model.forward(X_test)\n",
    "    preds = preds.reshape(-1, 1)\n",
    "    print(\"Mean absolute error: {:.2f}\".format(mae(preds, y_test)))\n",
    "    print()\n",
    "    print(\"Root mean squared error {:.2f}\".format(rmse(preds, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float = 1e-9):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.single_output = False\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        \n",
    "        softmax_pred = softmax(self.predictions, axis=1)\n",
    "        \n",
    "        # to prevent instability clipping the softmax outcomes\n",
    "        softmax_pred = np.clip(softmax_pred, self.eps, 1-self.eps)\n",
    "        \n",
    "        # loss computation\n",
    "        softmax_cross_entropy_loss = (\n",
    "            -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "                (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "        )\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss)\n",
    "    \n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return self.softmax_preds - self.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_model(model, test_set):\n",
    "    return print(f'''The model validation accuracy is: {np.equal(np.argmax(model.forward(test_set), axis=1), y_test).sum() * 100.0 / test_set.shape[0]:.2f}%''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = NeuralNetwork(\n",
    "    layers=[Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "nn = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")\n",
    "\n",
    "dl = NeuralNetwork(\n",
    "    layers=[Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=13,\n",
    "                   activation=Sigmoid()),\n",
    "            Dense(neurons=1,\n",
    "                   activation=Linear())],\n",
    "    loss=MeanSquaredError(),\n",
    "    seed=20190501\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data, train-test split etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tp/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "data = boston.data\n",
    "target = boston.target\n",
    "features = boston.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d_np(a: ndarray, \n",
    "          type: str=\"col\") -> ndarray:\n",
    "    '''\n",
    "    Turns a 1D Tensor into 2D\n",
    "    '''\n",
    "\n",
    "    assert a.ndim == 1, \\\n",
    "    \"Input tensors must be 1 dimensional\"\n",
    "    \n",
    "    if type == \"col\":        \n",
    "        return a.reshape(-1, 1)\n",
    "    elif type == \"row\":\n",
    "        return a.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36.2 15.  10.4 13.1 15.  13.5 24.8 24.8 29.1 34.9 25.  22.7 21.8 22.\n",
      " 14.3 19.5 48.3  8.5 35.2 23.2 21.7 21.1 16.2 23.3 26.4 14.   8.3 50.\n",
      " 11.8  7.  25.2  6.3 32.  50.  22.  19.1 28.7 37.  18.7 28.7 19.5 10.2\n",
      " 21.9 22.2 25.  22.1 21.2 19.2 17.5 19.6 30.8 39.8  9.7 30.7  7.4 24.\n",
      " 24.6 20.8 21.1 13.8 26.2 18.3 19.3 17.9 31.6 22.9 14.1 18.6 22.7 22.\n",
      " 50.  19.3 19.7 10.9 50.  23.6 50.   5.6 31.1 12.  22.4 27.1 30.3 23.1\n",
      " 23.3 22.6 36.2 17.8 34.9 16.8 17.2 24.  19.9 20.4 30.1 21.2 20.3 35.1\n",
      " 14.4 20.9 29.8 24.4 22.8 23.  50.  22.8 11.7 22.6 13.3 19.4 21.4 29.9\n",
      " 23.  22.8 20.6 20.3 33.  19.9 24.8 32.9 19.3 20.1 23.2 23.8 13.3 50.\n",
      " 22.5 16.5  5.  18.1 14.5 16.5 20.5 13.4 17.1 50.  18.4 17.3 23.9 18.8\n",
      " 20.3 32.5 24.1 23.1 18.9 43.8 21.4 31.6 33.3 20.6 17.8  7.2 20.6 24.7\n",
      " 20.2 36.  22.3 19.8 15.7  7.2 15.6 23.7 11.  13.4 19.6 14.4 19.4 19.1\n",
      " 13.2 18.6 27.5 36.4 23.3 31.5 21.6 24.4 22.5 24.3 16.7 19.6 31.7 21.9\n",
      " 21.  29.6 37.9 20.6 17.4 18.  15.6 34.6 23.8 22.  50.  23.9 33.8 21.4\n",
      " 10.4 28.5 17.5 14.9 13.  16.4 18.5 25.  24.7 29.  15.2 16.2 13.8 13.6\n",
      "  8.8 25.  20.4 18.5  8.8 14.1 33.1 22.9 23.8 16.3 21.7 12.6 13.8 22.6\n",
      " 14.5  8.4 20.9 26.4 14.2 19.4 13.4 33.4 20.1 50.  23.5 20.6 27.1 28.6\n",
      " 20.7 21.7 25.  32.2 30.1 19.1 43.1 15.4 24.5 18.4 26.6 27.5 37.6 17.4\n",
      " 21.8 25.1 26.6 26.7 24.5 23.1 22.5 21.  22.2 12.7 15.6 28.7 15.2 20.1\n",
      " 16.6 20.6 13.8 34.7 32.7 21.7 23.4 31.2 33.4  5.  19.1 13.1 29.6 24.6\n",
      " 24.8 17.8 14.6  7.2 26.6 11.9 24.1 22.4 12.7 44.8 14.1 37.2 24.1 24.3\n",
      " 18.5 20.  20.7 21.9 36.1 24.4 12.5 18.3 23.9 28.4 45.4 18.4 41.3 16.\n",
      " 20.1 23.1 29.8 31.5 25.  16.1 12.7 41.7 13.1 17.2 29.4 19.9 20.8 20.\n",
      " 12.8 14.8 21.2 22.6 21.5 27.5 11.3 30.5 27.9 35.4 22.2 22.8 17.4 50.\n",
      " 29.1 14.9 32.  19.9  7.5 22.  18.7 25.3 50.  28.1  8.7 13.8 19.5 19.\n",
      " 23.2 20.4 23.9 14.5]\n",
      "[[36.2]\n",
      " [15. ]\n",
      " [10.4]\n",
      " [13.1]\n",
      " [15. ]\n",
      " [13.5]\n",
      " [24.8]\n",
      " [24.8]\n",
      " [29.1]\n",
      " [34.9]\n",
      " [25. ]\n",
      " [22.7]\n",
      " [21.8]\n",
      " [22. ]\n",
      " [14.3]\n",
      " [19.5]\n",
      " [48.3]\n",
      " [ 8.5]\n",
      " [35.2]\n",
      " [23.2]\n",
      " [21.7]\n",
      " [21.1]\n",
      " [16.2]\n",
      " [23.3]\n",
      " [26.4]\n",
      " [14. ]\n",
      " [ 8.3]\n",
      " [50. ]\n",
      " [11.8]\n",
      " [ 7. ]\n",
      " [25.2]\n",
      " [ 6.3]\n",
      " [32. ]\n",
      " [50. ]\n",
      " [22. ]\n",
      " [19.1]\n",
      " [28.7]\n",
      " [37. ]\n",
      " [18.7]\n",
      " [28.7]\n",
      " [19.5]\n",
      " [10.2]\n",
      " [21.9]\n",
      " [22.2]\n",
      " [25. ]\n",
      " [22.1]\n",
      " [21.2]\n",
      " [19.2]\n",
      " [17.5]\n",
      " [19.6]\n",
      " [30.8]\n",
      " [39.8]\n",
      " [ 9.7]\n",
      " [30.7]\n",
      " [ 7.4]\n",
      " [24. ]\n",
      " [24.6]\n",
      " [20.8]\n",
      " [21.1]\n",
      " [13.8]\n",
      " [26.2]\n",
      " [18.3]\n",
      " [19.3]\n",
      " [17.9]\n",
      " [31.6]\n",
      " [22.9]\n",
      " [14.1]\n",
      " [18.6]\n",
      " [22.7]\n",
      " [22. ]\n",
      " [50. ]\n",
      " [19.3]\n",
      " [19.7]\n",
      " [10.9]\n",
      " [50. ]\n",
      " [23.6]\n",
      " [50. ]\n",
      " [ 5.6]\n",
      " [31.1]\n",
      " [12. ]\n",
      " [22.4]\n",
      " [27.1]\n",
      " [30.3]\n",
      " [23.1]\n",
      " [23.3]\n",
      " [22.6]\n",
      " [36.2]\n",
      " [17.8]\n",
      " [34.9]\n",
      " [16.8]\n",
      " [17.2]\n",
      " [24. ]\n",
      " [19.9]\n",
      " [20.4]\n",
      " [30.1]\n",
      " [21.2]\n",
      " [20.3]\n",
      " [35.1]\n",
      " [14.4]\n",
      " [20.9]\n",
      " [29.8]\n",
      " [24.4]\n",
      " [22.8]\n",
      " [23. ]\n",
      " [50. ]\n",
      " [22.8]\n",
      " [11.7]\n",
      " [22.6]\n",
      " [13.3]\n",
      " [19.4]\n",
      " [21.4]\n",
      " [29.9]\n",
      " [23. ]\n",
      " [22.8]\n",
      " [20.6]\n",
      " [20.3]\n",
      " [33. ]\n",
      " [19.9]\n",
      " [24.8]\n",
      " [32.9]\n",
      " [19.3]\n",
      " [20.1]\n",
      " [23.2]\n",
      " [23.8]\n",
      " [13.3]\n",
      " [50. ]\n",
      " [22.5]\n",
      " [16.5]\n",
      " [ 5. ]\n",
      " [18.1]\n",
      " [14.5]\n",
      " [16.5]\n",
      " [20.5]\n",
      " [13.4]\n",
      " [17.1]\n",
      " [50. ]\n",
      " [18.4]\n",
      " [17.3]\n",
      " [23.9]\n",
      " [18.8]\n",
      " [20.3]\n",
      " [32.5]\n",
      " [24.1]\n",
      " [23.1]\n",
      " [18.9]\n",
      " [43.8]\n",
      " [21.4]\n",
      " [31.6]\n",
      " [33.3]\n",
      " [20.6]\n",
      " [17.8]\n",
      " [ 7.2]\n",
      " [20.6]\n",
      " [24.7]\n",
      " [20.2]\n",
      " [36. ]\n",
      " [22.3]\n",
      " [19.8]\n",
      " [15.7]\n",
      " [ 7.2]\n",
      " [15.6]\n",
      " [23.7]\n",
      " [11. ]\n",
      " [13.4]\n",
      " [19.6]\n",
      " [14.4]\n",
      " [19.4]\n",
      " [19.1]\n",
      " [13.2]\n",
      " [18.6]\n",
      " [27.5]\n",
      " [36.4]\n",
      " [23.3]\n",
      " [31.5]\n",
      " [21.6]\n",
      " [24.4]\n",
      " [22.5]\n",
      " [24.3]\n",
      " [16.7]\n",
      " [19.6]\n",
      " [31.7]\n",
      " [21.9]\n",
      " [21. ]\n",
      " [29.6]\n",
      " [37.9]\n",
      " [20.6]\n",
      " [17.4]\n",
      " [18. ]\n",
      " [15.6]\n",
      " [34.6]\n",
      " [23.8]\n",
      " [22. ]\n",
      " [50. ]\n",
      " [23.9]\n",
      " [33.8]\n",
      " [21.4]\n",
      " [10.4]\n",
      " [28.5]\n",
      " [17.5]\n",
      " [14.9]\n",
      " [13. ]\n",
      " [16.4]\n",
      " [18.5]\n",
      " [25. ]\n",
      " [24.7]\n",
      " [29. ]\n",
      " [15.2]\n",
      " [16.2]\n",
      " [13.8]\n",
      " [13.6]\n",
      " [ 8.8]\n",
      " [25. ]\n",
      " [20.4]\n",
      " [18.5]\n",
      " [ 8.8]\n",
      " [14.1]\n",
      " [33.1]\n",
      " [22.9]\n",
      " [23.8]\n",
      " [16.3]\n",
      " [21.7]\n",
      " [12.6]\n",
      " [13.8]\n",
      " [22.6]\n",
      " [14.5]\n",
      " [ 8.4]\n",
      " [20.9]\n",
      " [26.4]\n",
      " [14.2]\n",
      " [19.4]\n",
      " [13.4]\n",
      " [33.4]\n",
      " [20.1]\n",
      " [50. ]\n",
      " [23.5]\n",
      " [20.6]\n",
      " [27.1]\n",
      " [28.6]\n",
      " [20.7]\n",
      " [21.7]\n",
      " [25. ]\n",
      " [32.2]\n",
      " [30.1]\n",
      " [19.1]\n",
      " [43.1]\n",
      " [15.4]\n",
      " [24.5]\n",
      " [18.4]\n",
      " [26.6]\n",
      " [27.5]\n",
      " [37.6]\n",
      " [17.4]\n",
      " [21.8]\n",
      " [25.1]\n",
      " [26.6]\n",
      " [26.7]\n",
      " [24.5]\n",
      " [23.1]\n",
      " [22.5]\n",
      " [21. ]\n",
      " [22.2]\n",
      " [12.7]\n",
      " [15.6]\n",
      " [28.7]\n",
      " [15.2]\n",
      " [20.1]\n",
      " [16.6]\n",
      " [20.6]\n",
      " [13.8]\n",
      " [34.7]\n",
      " [32.7]\n",
      " [21.7]\n",
      " [23.4]\n",
      " [31.2]\n",
      " [33.4]\n",
      " [ 5. ]\n",
      " [19.1]\n",
      " [13.1]\n",
      " [29.6]\n",
      " [24.6]\n",
      " [24.8]\n",
      " [17.8]\n",
      " [14.6]\n",
      " [ 7.2]\n",
      " [26.6]\n",
      " [11.9]\n",
      " [24.1]\n",
      " [22.4]\n",
      " [12.7]\n",
      " [44.8]\n",
      " [14.1]\n",
      " [37.2]\n",
      " [24.1]\n",
      " [24.3]\n",
      " [18.5]\n",
      " [20. ]\n",
      " [20.7]\n",
      " [21.9]\n",
      " [36.1]\n",
      " [24.4]\n",
      " [12.5]\n",
      " [18.3]\n",
      " [23.9]\n",
      " [28.4]\n",
      " [45.4]\n",
      " [18.4]\n",
      " [41.3]\n",
      " [16. ]\n",
      " [20.1]\n",
      " [23.1]\n",
      " [29.8]\n",
      " [31.5]\n",
      " [25. ]\n",
      " [16.1]\n",
      " [12.7]\n",
      " [41.7]\n",
      " [13.1]\n",
      " [17.2]\n",
      " [29.4]\n",
      " [19.9]\n",
      " [20.8]\n",
      " [20. ]\n",
      " [12.8]\n",
      " [14.8]\n",
      " [21.2]\n",
      " [22.6]\n",
      " [21.5]\n",
      " [27.5]\n",
      " [11.3]\n",
      " [30.5]\n",
      " [27.9]\n",
      " [35.4]\n",
      " [22.2]\n",
      " [22.8]\n",
      " [17.4]\n",
      " [50. ]\n",
      " [29.1]\n",
      " [14.9]\n",
      " [32. ]\n",
      " [19.9]\n",
      " [ 7.5]\n",
      " [22. ]\n",
      " [18.7]\n",
      " [25.3]\n",
      " [50. ]\n",
      " [28.1]\n",
      " [ 8.7]\n",
      " [13.8]\n",
      " [19.5]\n",
      " [19. ]\n",
      " [23.2]\n",
      " [20.4]\n",
      " [23.9]\n",
      " [14.5]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.3, random_state=80718)\n",
    "print(y_train)\n",
    "# make target 2d array\n",
    "y_train, y_test = to_2d_np(y_train), to_2d_np(y_test)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 30.293\n",
      "Validation loss after 20 epochs is 28.469\n",
      "Validation loss after 30 epochs is 26.293\n",
      "Validation loss after 40 epochs is 25.541\n",
      "Validation loss after 50 epochs is 25.087\n",
      "\n",
      "Mean absolute error: 3.52\n",
      "\n",
      "Root mean squared error 5.01\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(lr, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "eval_regression_model(lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 27.435\n",
      "Validation loss after 20 epochs is 21.839\n",
      "Validation loss after 30 epochs is 18.918\n",
      "Validation loss after 40 epochs is 17.195\n",
      "Validation loss after 50 epochs is 16.215\n",
      "\n",
      "Mean absolute error: 2.60\n",
      "\n",
      "Root mean squared error 4.03\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(nn, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "eval_regression_model(nn, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 44.143\n",
      "Validation loss after 20 epochs is 25.278\n",
      "Validation loss after 30 epochs is 22.339\n",
      "Validation loss after 40 epochs is 16.500\n",
      "Validation loss after 50 epochs is 14.655\n",
      "\n",
      "Mean absolute error: 2.45\n",
      "\n",
      "Root mean squared error 3.83\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(dl, SGD(lr=0.01))\n",
    "\n",
    "trainer.fit(X_train, y_train, X_test, y_test,\n",
    "       epochs = 50,\n",
    "       eval_every = 10,\n",
    "       seed=20190501);\n",
    "print()\n",
    "eval_regression_model(dl, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST with regular finctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tp/Jupyter/DLFS_code-master/lincoln\") # adding PATH to ./bashrc didn't work for me\n",
    "\n",
    "from lincoln.utils import mnist\n",
    "mnist.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 4 0 9 1 1 2 4 3 2 7 3 8 6 9 0 5 6\n",
      " 0 7 6 1 8 7 9 3 9 8 5 9 3 3 0 7 4 9 8 0 9 4 1 4 4 6 0 4 5 6 1 0 0 1 7 1 6\n",
      " 3 0 2 1 1 7 9 0 2 6 7 8 3 9 0 4 6 7 4 6 8 0 7 8 3 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, y_train, X_test, y_test = mnist.load()\n",
    "\n",
    "print(y_train[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode\n",
    "num_labels = len(y_train)\n",
    "train_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    train_labels[i][y_train[i]] = 1\n",
    "\n",
    "num_labels = len(y_test)\n",
    "test_labels = np.zeros((num_labels, 10))\n",
    "for i in range(num_labels):\n",
    "    test_labels[i][y_test[i]] = 1\n",
    "\n",
    "print(train_labels[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "[-33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145  17.68157855 125.68157855 219.68157855\n",
      " 125.68157855  16.68157855 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145  14.68157855\n",
      " 204.68157855 218.68157855 218.68157855 218.68157855 203.68157855\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145  20.68157855 193.68157855 219.68157855 218.68157855\n",
      " 205.68157855 199.68157855 218.68157855  23.68157855 -27.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -23.31842145  26.68157855 190.68157855\n",
      " 218.68157855 219.68157855 218.68157855 168.68157855  50.68157855\n",
      " 218.68157855 219.68157855  88.68157855 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " 129.68157855 218.68157855 218.68157855 218.68157855 219.68157855\n",
      " 218.68157855 218.68157855  62.68157855 155.68157855 219.68157855\n",
      " 133.68157855 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145  17.68157855 204.68157855 219.68157855\n",
      " 219.68157855 156.68157855  80.68157855 219.68157855 194.68157855\n",
      "  13.68157855  45.68157855 221.68157855 134.68157855 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145  14.68157855\n",
      " 204.68157855 218.68157855 218.68157855 145.68157855 -21.31842145\n",
      "  41.68157855  87.68157855 -12.31842145 -33.31842145 -33.31842145\n",
      " 219.68157855 209.68157855  16.68157855 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145   4.68157855 131.68157855 219.68157855 199.68157855\n",
      " 174.68157855  50.68157855 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 219.68157855 218.68157855\n",
      " 131.68157855 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -26.31842145 144.68157855\n",
      " 218.68157855 206.68157855  37.68157855 -14.31842145  -5.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 219.68157855 218.68157855 161.68157855 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145  23.68157855 218.68157855 218.68157855  29.68157855\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 219.68157855\n",
      " 218.68157855 161.68157855 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 164.68157855\n",
      " 219.68157855 156.68157855 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 221.68157855 219.68157855 162.68157855\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145  42.68157855 212.68157855 218.68157855  78.68157855\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " 219.68157855 218.68157855 114.68157855 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145  51.68157855\n",
      " 218.68157855 196.68157855  -8.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -26.31842145 101.68157855 219.68157855 152.68157855\n",
      " -21.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145  51.68157855 218.68157855 189.68157855\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -26.31842145  97.68157855\n",
      " 218.68157855 191.68157855  37.68157855 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      "  51.68157855 218.68157855 111.68157855 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      "  14.68157855 131.68157855 218.68157855 139.68157855 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145  52.68157855 219.68157855\n",
      " 191.68157855 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145  80.68157855 204.68157855 219.68157855\n",
      " 128.68157855 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145  51.68157855 218.68157855 215.68157855 112.68157855\n",
      "  14.68157855  -4.31842145  51.68157855 144.68157855 191.68157855\n",
      " 219.68157855 189.68157855 133.68157855  22.68157855 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145  51.68157855\n",
      " 218.68157855 218.68157855 218.68157855 195.68157855 181.68157855\n",
      " 218.68157855 218.68157855 218.68157855 162.68157855  96.68157855\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145  -5.31842145 165.68157855 218.68157855\n",
      " 218.68157855 219.68157855 218.68157855 218.68157855 199.68157855\n",
      " 111.68157855 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145  -8.31842145  94.68157855 218.68157855 219.68157855\n",
      " 218.68157855 107.68157855   3.68157855 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145 -33.31842145\n",
      " -33.31842145 -33.31842145 -33.31842145 -33.31842145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389  0.22504955  1.59966391  2.79608752  1.59966391  0.21232164\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.18686582  2.60516886\n",
      "  2.78335961  2.78335961  2.78335961  2.59244095 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389  0.26323329  2.46516184  2.79608752  2.78335961  2.61789677\n",
      "  2.54152931  2.78335961  0.30141702 -0.34770643 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.29679479  0.33960075  2.42697811\n",
      "  2.78335961  2.79608752  2.78335961  2.14696408  0.64507061  2.78335961\n",
      "  2.79608752  1.12873122 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389  1.65057556  2.78335961  2.78335961  2.78335961  2.79608752\n",
      "  2.78335961  2.78335961  0.79780554  1.98150124  2.79608752  1.7014872\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.22504955  2.60516886\n",
      "  2.79608752  2.79608752  1.99422915  1.02690793  2.79608752  2.47788976\n",
      "  0.17413791  0.58143105  2.82154335  1.71421511 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389  0.18686582  2.60516886  2.78335961  2.78335961  1.85422213\n",
      " -0.27133897  0.53051941  1.11600331 -0.15678777 -0.42407389 -0.42407389\n",
      "  2.79608752  2.66880842  0.21232164 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.05958671  1.67603138\n",
      "  2.79608752  2.54152931  2.22333154  0.64507061 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389  2.79608752  2.78335961\n",
      "  1.67603138 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.33497852  1.84149422  2.78335961  2.63062468  0.47960777\n",
      " -0.18224359 -0.06769239 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389  2.79608752  2.78335961  2.0578687  -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.30141702\n",
      "  2.78335961  2.78335961  0.37778448 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      "  2.79608752  2.78335961  2.0578687  -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389  2.09605243  2.79608752  1.99422915\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389  2.82154335  2.79608752\n",
      "  2.07059661 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      "  0.54324732  2.70699215  2.78335961  1.00145211 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389  2.79608752  2.78335961  1.4596569  -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.65779852  2.78335961\n",
      "  2.50334558 -0.10587613 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.33497852  1.29419406\n",
      "  2.79608752  1.9433175  -0.27133897 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389  0.65779852  2.78335961  2.4142502  -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.33497852  1.24328241  2.78335961  2.43970602  0.47960777\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      "  0.65779852  2.78335961  1.42147316 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.18686582  1.67603138\n",
      "  2.78335961  1.77785466 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389  0.67052643  2.79608752\n",
      "  2.43970602 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389  1.02690793  2.60516886  2.79608752  1.63784765 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389  0.65779852  2.78335961  2.74517588  1.43420107\n",
      "  0.18686582 -0.05496448  0.65779852  1.84149422  2.43970602  2.79608752\n",
      "  2.4142502   1.7014872   0.28868911 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      "  0.65779852  2.78335961  2.78335961  2.78335961  2.49061767  2.31242692\n",
      "  2.78335961  2.78335961  2.78335961  2.07059661  1.2305545  -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.06769239  2.10878034\n",
      "  2.78335961  2.78335961  2.79608752  2.78335961  2.78335961  2.54152931\n",
      "  1.42147316 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.10587613  1.20509868  2.78335961\n",
      "  2.79608752  2.78335961  1.37056152  0.0468588  -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389 -0.42407389\n",
      " -0.42407389 -0.42407389 -0.42407389 -0.42407389]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])\n",
    "X_train, X_test = X_train - np.mean(X_train), X_test - np.mean(X_train)\n",
    "print(X_train[1])\n",
    "X_train, X_test = X_train / np.std(X_train), X_test / np.std(X_train)\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST_NN = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89,\n",
    "               activation = Tanh()),\n",
    "           Dense(neurons = 10,\n",
    "                activation = Sigmoid())],\n",
    "        loss=MeanSquaredError(),\n",
    "        seed = 20190119)\n",
    "\n",
    "optimizer = SGD(lr = 0.1)\n",
    "\n",
    "\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            #batch_size=60\n",
    "           );\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.611\n",
      "Validation loss after 20 epochs is 0.426\n",
      "Validation loss after 30 epochs is 0.388\n",
      "Validation loss after 40 epochs is 0.374\n",
      "Validation loss after 50 epochs is 0.364\n",
      "The model validation accuracy is: 72.76%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=89,\n",
    "            activation=Tanh()),\n",
    "        Dense(neurons=10,\n",
    "            activation=Sigmoid())],\n",
    "    loss = MeanSquaredError(),\n",
    "    seed=20190119)\n",
    "optimizer = SGD(0.1)\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.576\n",
      "Validation loss after 20 epochs is 0.516\n",
      "Validation loss after 30 epochs is 0.462\n",
      "Validation loss after 40 epochs is 0.402\n",
      "Validation loss after 50 epochs is 0.371\n",
      "The model validation accuracy is: 73.21%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=110,\n",
    "            activation=Tanh()),\n",
    "        Dense(neurons=10,\n",
    "            activation=Sigmoid())],\n",
    "    loss = MeanSquaredError(),\n",
    "    seed=20190119)\n",
    "optimizer = SGD(0.1)\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 2.506\n",
      "Validation loss after 20 epochs is 2.457\n",
      "Validation loss after 30 epochs is 2.437\n",
      "Validation loss after 40 epochs is 2.425\n",
      "Validation loss after 50 epochs is 2.418\n",
      "The model validation accuracy is: 87.20%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=110,\n",
    "            activation=Tanh()),\n",
    "        Dense(neurons=10,\n",
    "            activation=Sigmoid())],\n",
    "    loss = SoftmaxCrossEntropy(),\n",
    "    seed=20190119)\n",
    "optimizer = SGD(0.1)\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.616\n",
      "Validation loss after 20 epochs is 0.566\n",
      "Validation loss after 30 epochs is 0.546\n",
      "Loss increased after epoch 40, final loss was 0.546, using the model from epoch 30\n",
      "The model validation accuracy is: 91.41%\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    layers=[Dense(neurons=110,\n",
    "            activation=Tanh()),\n",
    "        Dense(neurons=10,\n",
    "            activation=Linear())],\n",
    "    loss = SoftmaxCrossEntropy(),\n",
    "    seed=20190119)\n",
    "optimizer = SGD(0.1)\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(X_train, train_labels, X_test, test_labels,\n",
    "            epochs = 50,\n",
    "            eval_every = 10,\n",
    "            seed=20190119,\n",
    "            batch_size=60);\n",
    "\n",
    "calc_accuracy_model(model, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
